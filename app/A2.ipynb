{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import math\n",
    "import datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1+cpu\n",
      "Torchtext version: 0.16.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchtext version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd17985c1f0642f39edf49e6a00519c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\datasets--myothiha--starwars. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6131ea506402417891646d846229d01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)Heir to the Empire (by Timothy Zahn).txt:   0%|          | 0.00/687k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffbe3313f6943b094e04c22f92ccc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…) Dark Force Rising (by Timothy Zahn).txt:   0%|          | 0.00/764k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345d7971cdf4438683c9bca0a87f1964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)- The Last Command (by Timothy Zahn).txt:   0%|          | 0.00/820k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41478256454962b5e6b072bbef223a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e246f5618245d3835e6e772c49fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/8101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adae0947daff401aa008c49a50cf3da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('myothiha/starwars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7860\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9236\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7860, 1)\n",
      "(8101, 1)\n",
      "(9236, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)\n",
    "print(dataset['validation'].shape)\n",
    "print(dataset['test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0788f164183541369ba489702632e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f794c38653243c0ad9462f5af026668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5b765b5d294085bfe3a3e449428ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english') \n",
    "# set up a tokenizer: splitting the text into tokens by spaces and punctuation\n",
    "# e.g., Harry Potter > ['Harry', 'Potter']\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "# example: a single data entry (e.g., a sentence) from the dataset\n",
    "# tokenizer(example['text']) takes the text from the example and breaks it into tokens\n",
    "# the result is stored as 'tokens' in a dictionary\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "# dataset.map(tokenize_data) applies the tokenize_data function to every item in the dataset, tokenizing all sentences.\n",
    "# remove_columns=['text']: only need the tokenized wirds after tokenizing\n",
    "# fn_kwargs={'tokenizer': tokenizer} passes the tokenizer as a parameter to the tokenize_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', ',', 'i', 'see', ',', 'pellaeon', 'said', ',', 'not', 'entirely', 'truthfully', '.', 'admiral', ',', 'shouldn', \"'\", 't', 'we', 'be—', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][123]['tokens']) # tokens of the 123rd sentence in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "# build_vocab_from_iterator: creates a vocabulary (a list of unique words) from the tokens in the dataset.\n",
    "# tokenized_dataset['train']['tokens']: uses the tokens from the training set of the tokenized dataset.\n",
    "# min_freq=3: only includes words that appear at least 3 times in the dataset; otherwise ignore.\n",
    "\n",
    "\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "# add special tokens <unk> and <eos> for unknown and end of sentence\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "# if the model capture a word not in the vocab, it will use <unk> as an unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3449\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', ',', 'the', \"'\", 'to', 'a', 'of', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])\n",
    "# itos stands for Index-to-String. \n",
    "# get_itos() returns a list of tokens, mapping each index in the vocabulary to its corresponding word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset: # loop each example in the dataset\n",
    "        if example['tokens']: # check if the example contains any tokens\n",
    "            tokens = example['tokens'].append('<eos>') # add <eos> at the end\n",
    "            tokens = [vocab[token] for token in example['tokens']] # map each word in example['tokens'] to its index in vocab\n",
    "            data.extend(tokens) # add the elements of tokens to data instead of a single list of all elements.\n",
    "    data = torch.LongTensor(data) # convert the data list into a tensor\n",
    "    num_batches = data.shape[0] // batch_size # no. of batches\n",
    "    data = data[:num_batches * batch_size] # to confirm the no. of tokens and all batches have the same no. of tokens\n",
    "    data = data.view(batch_size, num_batches) # to reshape the data tensor\n",
    "    return data # [batch size, seq len] # seq len: no. of tokens in each sequence within a batch.\n",
    "\n",
    "# This function takes the dataset, tokenizes it, converts words to indices, \n",
    "# and then organizes the tokens into batches of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: torch.Size([32, 4619])\n",
      "Validation Data Shape: torch.Size([32, 5157])\n",
      "Test Data Shape: torch.Size([32, 5547])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Validation Data Shape: {valid_data.shape}\")\n",
    "print(f\"Test Data Shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self): # initialize weights for all layers (embedding, LSTM, fully connected layer)\n",
    "        init_range_emb = 0.1 # for embedding\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim) # for LSTM and fc\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other) # randomly in uniform dist.\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_() # initialize the bias of fc to 0\n",
    "        for i in range(self.num_layers): # loop each layer of the LSTM\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) # We # the weights between the embedding and LSTM layers\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) # Wh # the weights for the hidden-to-hidden connections within the LSTM\n",
    "    \n",
    "    def init_hidden(self, batch_size, device): # will be used when the model processes new batches\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device) # zeros tensor\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device) # zeros tensor\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden): # detach: the model will stop remembering the history of those states during the learning process\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() # not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        # src: [batch_size, seq len]\n",
    "       \n",
    "        embedding = self.dropout(self.embedding(src)) # e.g., harry potter is\n",
    "        # embedding: [batch-size, seq len, emb dim]\n",
    "        # dropout: to prevent overfitting\n",
    "\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        # output: [batch size, seq len, hid dim]\n",
    "        # hidden: [num_layers * direction, seq len, hid_dim]\n",
    "       \n",
    "        output = self.dropout(output) # to prevent overfitting\n",
    "       \n",
    "        prediction =self.fc(output)\n",
    "        # prediction: [batch_size, seq_len, vocab_size]\n",
    "       \n",
    "        return prediction, hidden\n",
    "        # return the predictions for the next word and the updated hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 23,860,601 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr) # Adam optimizer to adjust the model's weights during training\n",
    "criterion  = nn.CrossEntropyLoss() # loss function\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # the number of trainable parameters in the model\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    # data # [batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  # target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]  # we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    # reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) # src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        # need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  # prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1) # reshape into 1d\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward() # backpropagate\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # clip gradients to prevent explosion (limits gradients above a threshold)\n",
    "        optimizer.step() # update the model params using the optimizer\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches # avr loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad(): # disable gradient calculations to save memory and speed up evaluation\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1) # reshape into 1d\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches  # avr loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 175.991\n",
      "\tValid Perplexity: 120.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 127.953\n",
      "\tValid Perplexity: 99.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 105.477\n",
      "\tValid Perplexity: 88.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 92.029\n",
      "\tValid Perplexity: 82.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 82.017\n",
      "\tValid Perplexity: 78.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 74.724\n",
      "\tValid Perplexity: 75.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 68.778\n",
      "\tValid Perplexity: 73.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.752\n",
      "\tValid Perplexity: 71.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 59.157\n",
      "\tValid Perplexity: 70.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 55.362\n",
      "\tValid Perplexity: 70.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 51.672\n",
      "\tValid Perplexity: 70.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.035\n",
      "\tValid Perplexity: 68.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 44.624\n",
      "\tValid Perplexity: 68.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 42.425\n",
      "\tValid Perplexity: 67.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.371\n",
      "\tValid Perplexity: 67.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.374\n",
      "\tValid Perplexity: 67.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.161\n",
      "\tValid Perplexity: 67.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.655\n",
      "\tValid Perplexity: 67.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.152\n",
      "\tValid Perplexity: 67.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.732\n",
      "\tValid Perplexity: 67.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.519\n",
      "\tValid Perplexity: 67.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.449\n",
      "\tValid Perplexity: 67.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.325\n",
      "\tValid Perplexity: 67.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.173\n",
      "\tValid Perplexity: 67.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.163\n",
      "\tValid Perplexity: 67.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.011\n",
      "\tValid Perplexity: 67.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.238\n",
      "\tValid Perplexity: 67.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.120\n",
      "\tValid Perplexity: 67.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.121\n",
      "\tValid Perplexity: 67.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.067\n",
      "\tValid Perplexity: 67.399\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "seq_len  = 30 #<----decoding length\n",
    "clip    = 0.25 # if gradients exceed 0.25, they will be clipped.\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "# if the validation loss plateaus (doesn’t improve), the lr will be reduced by a factor of 0.5\n",
    "# patience=0: it will reduce the lr immediately when no improvement is seen\n",
    "\n",
    "best_valid_loss = float('inf') # intialize best val loss to inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 64.319\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens] # convert the list of tokens into indices\n",
    "    batch_size = 1 # generating text for a single prompt\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            # prediction: [batch size, seq len, vocab size]\n",
    "            # prediction[:, -1]: [batch size, vocab size] # probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: # if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    # if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) # autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices] # convert the list of indices back into the corresponding tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "yoda ' s voice .\n",
      "\n",
      "0.5\n",
      "yoda ' s eyes . it ' s no right , he said .\n",
      "\n",
      "0.75\n",
      "yoda ' s hand . i ' ve never met to yourself .\n",
      "\n",
      "0.8\n",
      "yoda ' s hand was close solidly .\n",
      "\n",
      "1.0\n",
      "yoda ' s quiet flight were than easing . he didn ' t look enough running enough to notice or secure kept this way . he glanced at lando . you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'yoda'\n",
    "max_seq_len = 30 # output including the given prompt\n",
    "seed = 0\n",
    "\n",
    "# smaller the temperature, more diverse tokens but comes with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.3, 0.5, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
